{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/langchain/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_sub = \"oracle_bert_sub\"\n",
    "model_path = \"oracle_bert\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/workspace/READ_SEQUENCE/DXDatav4.json\", \"r\", encoding = \"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "sub_labels_buffer = []\n",
    "\n",
    "for item in data:\n",
    "    for sub_item in item[\"RecordUtilSentenceGroupVoList\"]:\n",
    "        for sub_sub_item in sub_item[\"RecordUtilOracleCharVoList\"]:\n",
    "            sub_labels_buffer.append(sub_sub_item[\"SubLabel\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([[item, vlaue] for item, vlaue in Counter(sub_labels_buffer).items()]).to_csv(\"./sub_labels.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/workspace/READ_SEQUENCE/DXDatav4.json\", \"r\", encoding = \"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "labels_buffer = []\n",
    "\n",
    "for item in data:\n",
    "    for sub_item in item[\"RecordUtilSentenceGroupVoList\"]:\n",
    "        for sub_sub_item in sub_item[\"RecordUtilOracleCharVoList\"]:\n",
    "            labels_buffer.append(sub_sub_item[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([[item, vlaue] for item, vlaue in Counter(labels_buffer).items()]).to_csv(\"./labels.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speicial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "# 对sublabels进行去重\n",
    "labels = sorted([item for item in list(set(labels_buffer)) if item is not None])\n",
    "\n",
    "labels = labels\n",
    "\n",
    "with open(f\"./{model_path}/vocab.txt\", \"w\", encoding = \"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./oracle_bert/tokenizer_config.json',\n",
       " './oracle_bert/special_tokens_map.json',\n",
       " './oracle_bert/vocab.txt',\n",
       " './oracle_bert/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_token = BertTokenizer(\n",
    "    vocab_file = f\"./{model_path}/vocab.txt\", \n",
    "    do_lower_case = True, )\n",
    "\n",
    "bert_token.save_pretrained(f\"./{model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2 = bert_token([\"1\", \"r47d5160c7\"], is_split_into_words = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] r47d5160c7 [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_token.decode(input_2[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[1733, 1730, 1310, 1731, 1732],\n",
      "        [1733, 1730, 1734, 1310, 1731]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]])}\n",
      "['[CLS] [UNK] r47d5160c7 [SEP]', '[CLS] [UNK] r47d5160c7 [SEP]']\n",
      "['[CLS] [UNK] r47d5160c7 [SEP] [PAD]', '[CLS] [UNK] [MASK] r47d5160c7 [SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 输入一个BATCH的数据\n",
    "input_1 = bert_token(\n",
    "    [[\"dvfc5aavmn\", \"r47d5160c7\"],[\"dvfc5aavmn\", \"[MASK]\",\"r47d5160c7\"]], \n",
    "    is_split_into_words = True,\n",
    "    return_tensors = \"pt\",\n",
    "    padding = True)\n",
    "\n",
    "# 输入一条数据\n",
    "input_2 = bert_token([\"dvfc5aavmn\", \"r47d5160c7\"], is_split_into_words = True)\n",
    "print(input_1)\n",
    "\n",
    "batch_data = bert_token.batch_decode([input_2[\"input_ids\"], input_2[\"input_ids\"]])\n",
    "print(batch_data)\n",
    "\n",
    "batch_data = bert_token.batch_decode(input_1[\"input_ids\"])\n",
    "print(batch_data)\n",
    "# bert_token.convert_ids_to_tokens(outputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大序列长度: 278\n",
      "数据集长度: 10075\n"
     ]
    }
   ],
   "source": [
    "with open(\"./DXDatav4.json\", \"r\", encoding = \"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "datasets = []\n",
    "max_length = 0\n",
    "\n",
    "for item in data[2:]:\n",
    "    lines = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sub_item in item[\"RecordUtilSentenceGroupVoList\"]:\n",
    "        blocks = []\n",
    "        for sub_sub_item in sub_item[\"RecordUtilOracleCharVoList\"]:\n",
    "            blocks.append([sub_sub_item[\"Label\"], sub_sub_item[\"OrderNumber\"]])\n",
    "            \n",
    "        blocks = [item[0] for item in sorted(blocks, key = lambda x : x[-1])]\n",
    "        lengths.append(len(blocks))\n",
    "        lines.append(blocks)\n",
    "    if sum(lengths) > max_length:\n",
    "        max_length = sum(lengths)\n",
    "    datasets.append({\"name\": item['Facsimile'].split(\"/\")[-1], \"line\": lines})\n",
    "\n",
    "print(\"最大序列长度:\", max_length)\n",
    "print(\"数据集长度:\", len(datasets))\n",
    "\n",
    "with open(os.path.join(\"datasets\", \"oracle_lines.json\"), \"w\", encoding = \"utf-8\") as f:\n",
    "    json.dump(datasets, f, ensure_ascii = False, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_seqenceNone2UNK(sequences):\n",
    "    return [[item if item is not None else \"[UNK]\" for item in sequence] for sequence in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n",
      "2\n",
      "['[CLS] [UNK] [UNK] [UNK] [UNK] [SEP] [UNK] [UNK] [UNK] [UNK] [SEP]', '[CLS] [UNK] [UNK] [UNK] [MASK] [SEP] [UNK] [UNK] [UNK] [UNK] [SEP]']\n",
      "tensor([[1733, 1730, 1730, 1730, 1730, 1731, 1730, 1730, 1730, 1730, 1731],\n",
      "        [1733, 1730, 1730, 1730, 1734, 1731, 1730, 1730, 1730, 1730, 1731]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "{'input_ids': tensor([[1733, 1730, 1730, 1730, 1730, 1731, 1730, 1730, 1730, 1730, 1731],\n",
      "        [1733, 1730, 1730, 1730, 1734, 1731, 1730, 1730, 1730, 1730, 1731]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "seq1 = trans_seqenceNone2UNK(datasets[7])\n",
    "# input_sequences = \"[SEP]\".join([item for item in input_sequences])\n",
    "\n",
    "seq1 = \" [SEP] \".join([\" \".join(item) for item in seq1]).split(\" \")\n",
    "\n",
    "input_sequences = []\n",
    "input_sequences.append(seq1)\n",
    "\n",
    "seq2 = trans_seqenceNone2UNK(datasets[2])\n",
    "seq2 = \" [SEP] \".join([\" \".join(item) for item in seq2]).split(\" \")\n",
    "input_sequences.append(seq2)\n",
    "seq2[3] = \"[MASK]\"\n",
    "print(seq2[3])\n",
    "\n",
    "print(len(input_sequences))\n",
    "\n",
    "input_sequences_ = bert_token.__call__(input_sequences, \n",
    "           is_split_into_words = True, \n",
    "           return_tensors = \"pt\", \n",
    "           padding = True)\n",
    "\n",
    "print(bert_token.batch_decode(input_sequences_[\"input_ids\"]))\n",
    "\n",
    "\n",
    "print(input_sequences_[\"input_ids\"])\n",
    "print(input_sequences_['attention_mask'])\n",
    "print(input_sequences_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = BertConfig(\n",
    "    vocab_size = len(labels) + len(bert_token.get_added_vocab()),\n",
    "    position_embedding_type = \"absolute\",\n",
    "    pad_token_id = bert_token.get_added_vocab()[\"[PAD]\"],\n",
    "    num_hidden_layers = 6)\n",
    "\n",
    "bert_config.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1730"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(1735, 768, padding_idx=1732)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "inputs = bert_token([\"dvfc5aavmn\", \"r47d5160c7\"], return_tensors = \"pt\",is_split_into_words = True)\n",
    "\n",
    "pprint.pprint(model(**inputs)['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3125,  0.9100, -0.6813,  ...,  1.2145, -1.9278,  1.3722],\n",
       "         [-0.4977,  1.2471,  0.9930,  ..., -0.4758, -1.0121,  0.7602],\n",
       "         [-0.9555,  0.9318,  1.3791,  ...,  1.0507,  0.4240,  2.0551],\n",
       "         [ 0.0435,  1.3658,  0.5790,  ..., -0.5127, -2.1826,  0.7507]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"last_hidden_state\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
